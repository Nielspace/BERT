{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert=>Multitext Classification : Fine Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvnZud+bYMpK/PuQYXK0Tt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nielspace/BERT/blob/master/Bert%3D%3EMultitext_Classification_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wf0Ryu0lBot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "5b5df0d0-21f8-4c0d-eed9-191a251ac50b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 8.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 25.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=08b6f05498c87796fc336ae7250c57178f6176c6a511939d4085698f66655bb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB46f_GZkbJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "\n",
        "class config:\n",
        "    MODEL = 'bert-base-uncased'\n",
        "\n",
        "    HIDDEN = 768\n",
        "\n",
        "    MAX_LENGTH = 64\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VALID_BATCH_SIZE = 32\n",
        "\n",
        "    EPOCHS = 2\n",
        "\n",
        "    LR = (2e-5, 3e-5, 5e-5)\n",
        "    EPS = 1e-8\n",
        "\n",
        "    SEED = 23\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94u9uQANlJBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b13e589d-8eaa-49f2-fe2a-7980dffb71ac"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_json(\"News_Category_Dataset_v2.json\", lines=True)\n",
        "\n",
        "print(data)\n",
        "\n",
        "data = data.dropna()\n",
        "data = data.sample(n=20000)\n",
        "\n",
        "data['text'] = data.headline + data.short_description\n",
        "encoder = LabelEncoder()\n",
        "data[\"classes\"] = encoder.fit_transform((data['category']))\n",
        "data = data[['text', 'category', 'classes']]\n",
        "\n",
        "\n",
        "class TextClassificationDataset:\n",
        "\n",
        "    def __init__(self,\n",
        "                 texts,\n",
        "                 labels):\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_seq_length = config.MAX_LENGTH\n",
        "\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            config.MODEL\n",
        "        )\n",
        "\n",
        "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
        "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
        "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        texts = str(self.texts[item])\n",
        "        texts = \" \".join(texts.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            texts,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=self.max_seq_length,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        true_seq_length = len(inputs['input_ids'][0])\n",
        "        pad_size = self.max_seq_length - true_seq_length\n",
        "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
        "        ids = torch.cat((inputs['input_ids'][0], pad_ids))\n",
        "\n",
        "\n",
        "        output_dict = {\n",
        "            \"ids\": ids.flatten(),\n",
        "            'attention_mask': inputs[\"attention_mask\"][0].flatten(),\n",
        "            'target' : torch.tensor(self.labels[item], dtype=torch.long)\n",
        "\n",
        "        }\n",
        "\n",
        "        \n",
        "\n",
        "        return output_dict\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "n_classes = len(data['classes'].unique())\n",
        "\n",
        "train, val = train_test_split(\n",
        "    data, test_size=0.30, random_state=SEED)\n",
        "\n",
        "\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "val.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             category  ...       date\n",
            "0               CRIME  ... 2018-05-26\n",
            "1       ENTERTAINMENT  ... 2018-05-26\n",
            "2       ENTERTAINMENT  ... 2018-05-26\n",
            "3       ENTERTAINMENT  ... 2018-05-26\n",
            "4       ENTERTAINMENT  ... 2018-05-26\n",
            "...               ...  ...        ...\n",
            "200848           TECH  ... 2012-01-28\n",
            "200849         SPORTS  ... 2012-01-28\n",
            "200850         SPORTS  ... 2012-01-28\n",
            "200851         SPORTS  ... 2012-01-28\n",
            "200852         SPORTS  ... 2012-01-28\n",
            "\n",
            "[200853 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiFwRxyHlM2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(config.MODEL)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(config.HIDDEN, n_classes)\n",
        "\n",
        "            \n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        _, pooled_output = self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "        output = self.drop(pooled_output)\n",
        "        # output = self.softmax(output)\n",
        "        return self.out(output)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTWmhqnjnp0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "ac3828ef-a6a6-4255-f177-91b1cdccd46d"
      },
      "source": [
        "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "traindata = TextClassificationDataset(\n",
        "    train.text.to_numpy(), \n",
        "    train.classes.to_numpy()\n",
        "    )\n",
        "\n",
        "trainLoader = DataLoader(\n",
        "    traindata, \n",
        "    shuffle=True, \n",
        "    batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "\n",
        "valdata = TextClassificationDataset(\n",
        "    val.text.to_numpy(), \n",
        "    train.classes.to_numpy()\n",
        "    )\n",
        "\n",
        "valLoader = DataLoader(\n",
        "    valdata, \n",
        "    shuffle=True, \n",
        "    batch_size=config.TRAIN_BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print(\"Cuda available\")\n",
        "\n",
        "else:\n",
        "    print(\"No GPU's available\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "TOTAL_STEPS = len(trainLoader)*config.EPOCHS\n",
        "\n",
        "model = Model(n_classes)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = config.LR[-1],\n",
        "    eps=config.EPS\n",
        "    )\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "def accuracy_check(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    label_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == label_flat) / len(label_flat)\n",
        "\n",
        "\n",
        "def format_time(elasped):\n",
        "    elasped_rounded = int(round(elasped))\n",
        "\n",
        "    return str(datetime.timedelta(seconds=elasped_rounded))\n",
        "\n",
        "\n",
        "random.seed(config.SEED)\n",
        "np.random.seed(config.SEED)\n",
        "torch.manual_seed(config.SEED)\n",
        "torch.cuda.manual_seed(config.SEED)\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return nn.CrossEntropyLoss()(outputs, targets.view(-1, 1))\n",
        "\n",
        "for epoch in range(0, config.EPOCHS):\n",
        "    print(\"\")\n",
        "    print(\"============EPOCHS {:}/{:}=============\".format(epoch + 1, config.EPOCHS))\n",
        "    print(\"Training\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(trainLoader):\n",
        "       \n",
        "\n",
        "        if step % 40 == 0 and not step == 0: \n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "\n",
        "            print(\" Batch {:>5,} of {:>5,}. Elasped: {:}\".format(step, len(trainLoader), elapsed))\n",
        "\n",
        "        b_input_ids = batch['ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['target'].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(\n",
        "            b_input_ids,\n",
        "            attention_mask = b_input_mask,\n",
        "            )\n",
        "\n",
        "        # output = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "        # print(output)\n",
        "\n",
        "        # lossfn = torch.nn.CrossEntropyLoss().to(device)\n",
        "        loss = lossfn(output, b_labels).to(device)\n",
        "        \n",
        "        total_train_loss+=loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        avg_train_loss = total_train_loss/len(trainLoader)\n",
        "\n",
        "\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    #=========================================\n",
        "    #           Validation\n",
        "    #=========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation\")\n",
        "    \n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_step = 0\n",
        "\n",
        "    for batch in valLoader:\n",
        "\n",
        "        b_input_ids = batch['ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['target'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.zero_grad()\n",
        "\n",
        "            output = model(\n",
        "                b_input_ids,\n",
        "                attention_mask = b_input_mask,\n",
        "                )\n",
        "\n",
        "        # output = torch.nn.functional.softmax(output)\n",
        "\n",
        "        # lossfn = torch.nn.CrossEntropyLoss().to(device)\n",
        "        loss = lossfn(output, b_labels).to(device)\n",
        "        \n",
        "        total_eval_loss+=loss.item()\n",
        "\n",
        "        output = output.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        total_eval_accuracy += accuracy_check(output, label_ids)\n",
        "\n",
        "        \n",
        "    \n",
        "    avg_val_accuracy = total_eval_accuracy/len(valLoader)\n",
        "\n",
        "    print(\"   Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss/len(valLoader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            \"epochs\":epoch + 1,\n",
        "            \"Trainning Loss\":avg_train_loss,\n",
        "            \"Valid Loss\": avg_val_loss,\n",
        "            \"Valid Acc\": avg_val_accuracy,\n",
        "            \"Training Time\": validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training Complete\")\n",
        "print(\"Total Time took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "        \n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available\n",
            "\n",
            "============EPOCHS 1/2=============\n",
            "Training\n",
            " Batch    40 of   438. Elasped: 0:00:14\n",
            " Batch    80 of   438. Elasped: 0:00:28\n",
            " Batch   120 of   438. Elasped: 0:00:43\n",
            " Batch   160 of   438. Elasped: 0:00:57\n",
            " Batch   200 of   438. Elasped: 0:01:11\n",
            " Batch   240 of   438. Elasped: 0:01:26\n",
            " Batch   280 of   438. Elasped: 0:01:41\n",
            " Batch   320 of   438. Elasped: 0:01:55\n",
            " Batch   360 of   438. Elasped: 0:02:10\n",
            " Batch   400 of   438. Elasped: 0:02:25\n",
            "\n",
            "  Average training loss: 2.15\n",
            "  Training epoch took: 0:02:40\n",
            "\n",
            "Running Validation\n",
            "   Accuracy: 0.07\n",
            "  Validation Loss: 4.79\n",
            "  Validation took: 0:00:25\n",
            "\n",
            "============EPOCHS 2/2=============\n",
            "Training\n",
            " Batch    40 of   438. Elasped: 0:00:15\n",
            " Batch    80 of   438. Elasped: 0:00:31\n",
            " Batch   120 of   438. Elasped: 0:00:46\n",
            " Batch   160 of   438. Elasped: 0:01:02\n",
            " Batch   200 of   438. Elasped: 0:01:17\n",
            " Batch   240 of   438. Elasped: 0:01:33\n",
            " Batch   280 of   438. Elasped: 0:01:49\n",
            " Batch   320 of   438. Elasped: 0:02:04\n",
            " Batch   360 of   438. Elasped: 0:02:20\n",
            " Batch   400 of   438. Elasped: 0:02:36\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epoch took: 0:02:50\n",
            "\n",
            "Running Validation\n",
            "   Accuracy: 0.07\n",
            "  Validation Loss: 5.25\n",
            "  Validation took: 0:00:26\n",
            "\n",
            "Training Complete\n",
            "Total Time took 0:06:21 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkIKDfFqrsww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8de2859b-f51b-40c3-b093-9d10483f41e2"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 41])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AofquPmOxxm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}